# Phase 2 Implementation Plan: Voice & AI Intelligence

This document outlines the detailed steps for implementing Phase 2 of the AI Music Looper Agent. The primary goals are to integrate voice for command input and spoken feedback, and to upgrade the Python agent to understand natural language for more complex track manipulation.

## 1. Core Objectives

-   **Voice Input:** Replace manual text/curl commands with real-time speech-to-text using `SFSpeechRecognizer`.
-   **Voice Output:** Enable the agent to provide spoken feedback using `AVSpeechSynthesizer`.
-   **Natural Language Understanding (NLU):** Upgrade the Python agent's `RouterNode` to use an LLM, allowing it to interpret natural language commands (e.g., "make the guitar loop quieter").
-   **Stateful Track Manipulation:** Implement the ability to modify specific properties of individual tracks, starting with volume.

## 2. Phase 2 Architecture

The hybrid architecture established in previous phases remains, but the communication becomes more sophisticated.

1.  **Swift App (`ContentView` & `SpeechManager`):**
    - Captures audio via a "push-to-talk" mechanism.
    - Uses `SFSpeechRecognizer` to transcribe the user's speech to text.
    - Sends the transcribed text to the Python backend via `AgentClient`.
    - Receives a JSON response from the backend.
    - If the response contains a `speak` action, `AVSpeechSynthesizer` speaks the text.
    - If the response contains an `audio_engine` action, `AudioManager` executes it.

2.  **Python Backend (`main.py`):**
    - The FastAPI server receives the text command.
    - The `LangGraph` agent processes the text.
    - The `RouterNode` now uses an LLM to determine intent and extract entities (e.g., action: `set_volume`, target: `track_1`, value: `0.5`).
    - The graph directs the flow to a new `ModifyTrackNode` for valid modification commands.
    - The agent constructs a JSON response containing either a command for the `AVAudioEngine` or text for the `AVSpeechSynthesizer`.

## 3. Implementation Steps

### Part 1: Python Backend - Agent Intelligence

The Python backend needs to become smarter. We'll upgrade it from a simple keyword-based router to an LLM-powered one. All work will be done within the `python/` directory.

**File to be Modified: `python/main.py`**

#### **Step 1.1: Enhance the LangGraph State**

The agent's state needs to store information about each track.

```python
# python/main.py

# ... existing imports ...
from typing import List, Dict, Any

class Track:
    id: str
    name: str # e.g., "guitar_1", "drums"
    volume: float
    # Future properties: effects, file_path, etc.

class AgentState(TypedDict):
    tracks: List[Track]
    # ... other state variables
```

#### **Step 1.2: Upgrade the Router Node**

This is the most critical change. The router will now use a function-calling LLM to parse the user's command.

```python
# python/main.py

# ... imports for langchain/llm ...

def router_node(state: AgentState):
    """
    Uses an LLM to interpret the user's command and route to the correct node.
    """
    user_command = state["command"] # Assuming command is passed in state
    
    # Define "tools" or "functions" that the LLM can call
    # This provides the structured output we need.
    tools = [
        # Functions for recording, stopping, etc.
        # NEW: Function for modifying a track
        {
            "name": "modify_track",
            "description": "Modify a property of a specific track, like volume.",
            "parameters": {
                "type": "object",
                "properties": {
                    "track_id": {"type": "string", "description": "The ID of the track to modify."},
                    "volume": {"type": "number", "description": "The new volume level (0.0 to 1.0)."}
                },
                "required": ["track_id"]
            }
        }
    ]

    # Use a LangChain model that supports function calling (e.g., from OpenAI, Anthropic, or a local model)
    # llm_with_tools = llm.bind_tools(tools)
    # result = llm_with_tools.invoke(user_command)
    
    # Logic to check the LLM's output and decide which node to go to next.
    # if result.tool_calls and result.tool_calls[0]['name'] == 'modify_track':
    #     # Add parsed arguments to state and return "modify_track"
    #     return "modify_track_node"
    # else:
    #     # Route to other nodes (record, etc.) or a fallback
    #     return "fallback_node"
```

#### **Step 1.3: Create the `ModifyTrackNode`**

This new node will process the structured data from the router and prepare the command for Swift.

```python
# python/main.py

def modify_track_node(state: AgentState):
    """
    Updates the state and prepares the command for the Swift audio engine.
    """
    # Arguments are already parsed and in the state from the router
    track_id = state["modification_args"]["track_id"]
    new_volume = state["modification_args"]["volume"]

    # Update the internal state
    for track in state["tracks"]:
        if track.id == track_id:
            track.volume = new_volume
            break
            
    # Prepare the command for Swift
    response = {
        "action": "set_volume",
        "track_id": track_id,
        "volume": new_volume
    }
    state["response"] = response
    return state
```

### Part 2: Swift Frontend - Voice & Audio Control

Now, we'll build the native components for voice and enhance the audio engine.

#### **Step 2.1: Implement the Speech Manager**

Create a new file to handle all `SFSpeechRecognizer` and `AVSpeechSynthesizer` logic.

**New File: `JamSession3/SpeechManager.swift`**

```swift
// JamSession3/SpeechManager.swift

import Foundation
import Speech
import AVFoundation

class SpeechManager: NSObject, ObservableObject {
    @Published var transcribedText: String = ""
    
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "en-US"))!
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let audioEngine = AVAudioEngine()
    
    private let speechSynthesizer = AVSpeechSynthesizer()

    func requestPermission() {
        SFSpeechRecognizer.requestAuthorization { authStatus in
            // Handle authorization status
        }
    }

    func startTranscribing() {
        // ... Implementation for starting audio session and recognition task ...
    }

    func stopTranscribing() {
        // ... Implementation for stopping transcription ...
    }
    
    func speak(_ text: String) {
        let utterance = AVSpeechUtterance(string: text)
        utterance.voice = AVSpeechSynthesisVoice(language: "en-US")
        speechSynthesizer.speak(utterance)
    }
}
```

#### **Step 2.2: Enhance the Audio Manager**

Add the new function to control the volume of a specific track.

**File to be Modified: `JamSession3/AudioManager.swift`**

```swift
// JamSession3/AudioManager.swift

// ... existing code ...
class AudioManager {
    // ... existing properties (engine, playerNodes, etc.)
    
    // Assume playerNodes is a dictionary: [String: AVAudioPlayerNode]
    
    func setVolume(forTrack trackID: String, volume: Float) {
        guard let playerNode = playerNodes[trackID] else {
            print("Error: Track with ID \(trackID) not found.")
            return
        }
        playerNode.volume = volume
        print("Set volume for track \(trackID) to \(volume)")
    }
    
    // ... other existing functions (startRecording, stopRecordingAndCreateLoop)
}
```

#### **Step 2.3: Update the Agent Client**

Modify the client to handle the new, more complex JSON responses.

**File to be Modified: `JamSession3/AgentClient.swift`**

```swift
// JamSession3/AgentClient.swift

// Update the Response structure
struct AgentResponse: Codable {
    let action: String?
    let speak: String?
    let track_id: String?
    let volume: Float?
}

// Update the postCommand function's completion handler
// func postCommand(command: String, completion: @escaping (AgentResponse) -> Void)

// In the completion handler, you'll now check for different fields
// if let textToSpeak = response.speak { speechManager.speak(textToSpeak) }
// if let action = response.action { /* call audio manager */ }
```

#### **Step 2.4: Integrate into the UI**

Update the main view to include a "push-to-talk" button and connect all the new pieces.

**File to be Modified: `JamSession3/ContentView.swift`**

```swift
// JamSession3/ContentView.swift

struct ContentView: View {
    @StateObject private var speechManager = SpeechManager()
    @StateObject private var audioManager = AudioManager() // Assuming this is already here
    @StateObject private var agentClient = AgentClient() // Assuming this is already here
    
    @State private var isRecordingSpeech = false

    var body: some View {
        VStack {
            // ... other UI elements ...
            
            Text(speechManager.transcribedText)
            
            Button(action: {
                if isRecordingSpeech {
                    speechManager.stopTranscribing()
                    // Send transcribed text to agent
                    agentClient.postCommand(command: speechManager.transcribedText) { response in
                        // Handle response from agent client
                    }
                } else {
                    speechManager.startTranscribing()
                }
                isRecordingSpeech.toggle()
            }) {
                Text(isRecordingSpeech ? "Stop Listening" : "Push to Talk")
            }
        }
        .onAppear(perform: speechManager.requestPermission)
    }
}
```

## 4. Testing & Verification

1.  **Speak "record a loop."**
    -   **Expected:** The app starts recording audio via `AVAudioEngine`. After stopping, the loop begins to play. The Python agent's state should now contain one track.
2.  **Speak "make that loop quieter."**
    -   **Expected:**
        -   The `SFSpeechRecognizer` transcribes the text.
        -   The Python agent's LLM router interprets this as a `modify_track` command with a volume likely around `0.5` or `0.7`.
        -   The agent returns a JSON response like `{"action": "set_volume", "track_id": "track_0", "volume": 0.7}`.
        -   The Swift `AudioManager` receives this command and lowers the volume on the corresponding `AVAudioPlayerNode`. The change should be audible.
3.  **Speak a command the agent doesn't understand (e.g., "what's the weather?").**
    -   **Expected:** The Python agent's router fails to match this to any function. It should fall back to a response like `{"speak": "I'm sorry, I can only help with music."}`. The Swift app should speak this sentence aloud using `AVSpeechSynthesizer`. 