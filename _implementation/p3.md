# Phase 3 Implementation Plan: Advanced Features & Creative Tools

This document outlines the detailed plan for implementing Phase 3 of the AI Music Looper Agent. The goal of this phase is to build upon the core functionality and introduce advanced creative tools that make the application powerful and unique.

## Guiding Principles for Phase 3

*   **Component-Driven Development:** Each feature (Music Generation, Undo, etc.) will be developed as a modular component with clear responsibilities.
*   **API-First for Python:** The Python agent's new capabilities will be exposed through clean, well-defined endpoints on the FastAPI server.
*   **State-Driven UI:** The SwiftUI UI will be a direct, read-only reflection of the agent's state, updated via responses from the backend.

---

## 1. Music Generation

This feature allows the user to ask the agent to add new musical parts, which will be generated by a cloud-based AI model.

#### **Python Backend (LangGraph)** - ✅ **Completed (Placeholder)**

1.  **`MusicGenerationNode` Implementation:** ✅
    *   Create a new node file `python/nodes/music_generation_node.py`.
    *   This node will be responsible for calling an external music generation service. For the initial implementation, we can use a placeholder that returns a pre-recorded audio file to simulate the API call.
    *   The node function will take the current `State` as input to extract context like tempo (BPM) and musical key.
2.  **API Integration:** ✅
    *   Research and select a suitable music generation API (e.g., a model from Hugging Face, or another specialized service).
    *   Implement the API client logic within the `MusicGenerationNode` to send a request with the correct parameters.
    *   Handle the API response, which will likely be an audio file (e.g., MP3 or WAV).
3.  **State Management:** ✅
    *   When the generated audio is received, the node will save it to a temporary file, similar to how recorded loops are handled.
    *   It will then update the main `State` object by adding a new `Track` to the tracklist, including its ID, a name (e.g., "AI Bassline"), and the path to the audio file.
4.  **Agent Integration:** ✅
    *   Add the `MusicGenerationNode` to the main LangGraph graph.
    *   Update the `RouterNode`'s LLM prompt to recognize intents like "add a drum beat" or "generate a melody" and route the workflow to the `MusicGenerationNode`.

#### **Swift Frontend (macOS)** - ✅ **Completed**

1.  **Triggering Generation:** ✅
    *   No direct UI changes are needed to trigger this. The user's voice command, transcribed by `SFSpeechRecognizer`, will be sent to the Python backend.
2.  **Handling New Tracks:** ✅
    *   The backend's response will update the Swift app's local state. The existing logic that handles adding new recorded tracks to the `AVAudioEngine` will be used to add the new AI-generated track. The track will be loaded from its file path and scheduled for playback.

#### **Testing & Verification:**

*   Speak "add a simple drum beat".
*   **Expected Outcome:** After a brief processing period, a new drum loop track is added to the session and begins playing in sync with other tracks. The UI should update to show the new track.
*   **Architectural Note:** This node must be updated to commit its final state change using the `HistoryManager`.

---

## 2. Undo/Redo Functionality (Pragmatic Compromise)

This feature implements a robust, future-proof undo system. We will build the backend to support a full branching undo tree from day one, but for Phase 3, we will only expose simple, linear undo functionality to the user. This avoids a costly future refactor.

#### **Python Backend (LangGraph)** - ✅ **Completed**

1.  **`HistoryManager` Implementation:** ✅
    *   Create a new Python class `python/history_manager.py`.
    *   This class will manage the state history. It will use a simple SQLite database to store the state tree.
    *   The database will have a `state_tree` table with columns: `node_id` (PK), `parent_id` (FK to self), and `state_snapshot` (JSON blob).
    *   The `HistoryManager` will have a method: `commit(state, parent_id) -> new_id`. This writes a new state to the DB.
2.  **State Commits:** ✅
    *   All nodes that modify the application state (e.g., `MusicGenerationNode`, `ModifyTrackNode`) will no longer mutate the state directly. At the end of their execution, they must call `history_manager.commit()` to create a new state object.
3.  **`UndoNode` Implementation:** ✅
    *   Create an `UndoNode`. This node will not use LangGraph's checkpointer.
    *   It will call the `HistoryManager` to find the parent of the current state node and will load that parent's `state_snapshot`.
    *   This new (old) state will be passed back to the frontend, effectively performing an "undo".
4.  **Agent Integration:** ✅
    *   Update the `RouterNode` to recognize the "undo" intent and route to the `UndoNode`. The response to the frontend will contain the entire reverted state object.

#### **Swift Frontend (macOS)** - ✅ **Completed**

1.  **State Synchronization:** ✅
    *   This remains unchanged. When the Swift app receives the `newState` from the backend after an undo operation, it will replace its local state representation and re-configure the `AVAudioEngine` and UI to match. The complexity of the history tree is completely abstracted away by the backend.

#### **Testing & Verification:**

*   Record a new track.
*   Speak "undo that".
*   **Expected Outcome:** The last recorded track stops playing and is removed from the UI. The state is reverted to before the recording. The underlying database should show two states, with a parent-child relationship.

---

## 3. Creative Suggestions

The agent will be able to analyze the current musical session and provide intelligent suggestions for what to add next.

#### **Python Backend (LangGraph)** - ✅ **Completed (Placeholder)**

1.  **`AnalysisNode` Implementation:** ✅
    *   Create an `AnalysisNode`.
    *   This node will read the current `State` and generate a concise, human-readable summary of the session (e.g., "This is a 120 BPM rock track in E minor with two guitars and a drum beat.").
2.  **`SuggestionNode` Implementation:** ✅
    *   Create a `SuggestionNode` that takes the summary from the `AnalysisNode` as input.
    *   It will use an LLM (e.g., via a LangChain chain) to generate creative suggestions based on the summary. The prompt will be engineered to elicit musically relevant ideas (e.g., "Given the following track, suggest a new instrument to add: ...").
3.  **Agent Integration:** ✅
    *   Add the new nodes to the graph. The `RouterNode` will direct "what should I add?" intents to the `AnalysisNode`, which then flows to the `SuggestionNode`.
    *   The agent's final response will be a JSON payload containing the text to be spoken by the frontend (e.g., `{"speak": "How about adding a funky bassline?"}`).

#### **Swift Frontend (macOS)** - ✅ **Completed**

1.  **Spoken Response:** ✅
    *   The existing `AVSpeechSynthesizer` integration will handle speaking the agent's suggestion to the user. No new Swift code is required for this part.

#### **Testing & Verification:**

*   Create a session with 2-3 tracks.
*   Speak "what would sound good here?".
*   **Expected Outcome:** The agent provides a spoken, relevant musical suggestion.

---

## 4. Audio Effects

Users will be able to apply common audio effects like reverb and delay to specific tracks.

#### **Swift Frontend (macOS)** - ✅ **Completed**

1.  **`AVAudioEngine` Enhancements:** ✅
    *   For each track's `AVAudioPlayerNode`, attach `AVAudioUnitReverb` and `AVAudioUnitDelay` nodes between the player and the main mixer.
    *   Initially, set the `wetDryMix` for these effects to 0 so they are inactive.
2.  **Expose Effect Controls:** ✅
    *   Update the `AudioManager` to include functions like `setReverb(trackID: String, value: Float)` and `setDelay(trackID: String, value: Float)`. These functions will adjust the `wetDryMix` property of the corresponding audio unit.
3.  **Agent Command Handling:** ✅
    *   Enhance the network client's command handler to recognize new actions from the backend, such as `{"action": "set_reverb", "track_id": "track_001", "value": 50.0}`.

#### **Python Backend (LangGraph)** - ✅

1.  **Agent Logic:**
    *   Update the `RouterNode`'s LLM prompt to understand commands like "put some reverb on the first guitar" or "add a little delay to the last loop".
    *   The LLM must be prompted to extract the track identifier and the effect being requested.
    *   The `ModifyTrackNode` will be updated to handle these new action types and return the appropriate JSON command for the Swift frontend. This node must commit its final state using the `HistoryManager`.

#### **Testing & Verification:**

*   Record a guitar loop.
*   Speak "add a lot of reverb to that guitar".
*   **Expected Outcome:** The reverb effect is audibly applied to the guitar track.
*   **Architectural Note:** This node must commit its final state using the `HistoryManager`.

---

## 5. Basic User Interface

A minimal UI will be created to provide visual feedback of the agent's state.

#### **Swift Frontend (macOS)** - ✅ **Completed**

1.  **`TrackView` Component:** ✅
    *   Create a new SwiftUI view, `TrackView.swift`, that displays information for a single track (e.g., name, volume, mute status).
2.  **Main `ContentView` Layout:** ✅
    *   Modify `ContentView.swift` to display a list of `TrackView` components based on the tracks in the app's state.
    *   Use a `VStack` or `List` to arrange the tracks.
3.  **Status Indicators:** ✅
    *   Add a "listening" indicator (e.g., a colored circle) that activates when `SFSpeechRecognizer` is active.
    *   Implement the dedicated Record/Stop toggle button as defined in the project overview. This button will directly call the `startRecording()` and `stopRecording()` functions in the `AudioManager`.

#### **Python Backend (LangGraph)** - ✅ **Completed**

1.  **State Broadcasting:** ✅
    *   Ensure that every response from the agent that modifies the state includes the complete, updated `State` object. This is the single source of truth for the UI.

#### **Testing & Verification:**

*   Launch the application.
*   **Expected Outcome:** The main window appears with an empty state.
*   Record a loop.
*   **Expected Outcome:** A new track visualization appears in the UI.
*   Press the record button.
*   **Expected Outcome:** A "recording" indicator appears. Press it again, and the indicator disappears. 

---

## Implementation Notes & Findings

This section documents key architectural decisions, bugs encountered, and lessons learned during the implementation of Phase 3. It serves as a reference for future development.

### Backend (Python/LangGraph)

*   **Undo/Redo Architecture:** We successfully implemented the "pragmatic compromise." The `HistoryManager` uses a SQLite database and is built to support a full branching undo tree. However, we only exposed a linear `undo` command to the frontend, simplifying the user experience for now and avoiding a costly future refactor.
*   **History ID Management:** A critical bug was resolved where the frontend was not correctly receiving or sending the `history_node_id`. This was fixed by redesigning the `HistoryManager` to encapsulate all ID generation, making the process more robust and removing ambiguity.
*   **State Consistency:** We fixed a subtle bug where the state saved to the database was inconsistent, causing `undo` to fail. This highlighted the importance of ensuring that the state object passed to `history.commit()` is the *final, complete* state for that step.
*   **Circular Reference Crash:** We encountered and fixed a `RecursionError` crash. This was caused by the `undo_node`'s response including the entire state object, which contained a self-referential structure that `json.dumps` could not handle. The fix was to construct a clean, non-circular response dictionary.
*   **Initial State Handling:** The frontend sends an empty command on startup to fetch the initial state. This was initially routed to the `fallback_node`, causing an "I'm not sure how to do that" message. We added a `no_op_node` (no-operation) to handle this gracefully.
*   **Graph Registration Bug:** A persistent issue occurred where the newly created `no_op_node` was not correctly registered in the LangGraph `builder`. This caused a crash even though the router correctly identified the node. This required a manual fix to the `create_graph` function in `main.py` to add the node and its corresponding conditional edge.

### Frontend (Swift/SwiftUI)

*   **Centralized State Management:** The most significant architectural change was the creation of `AppState.swift`. This class now acts as the single source of truth for the application's state, decoupling the UI (`ContentView`) from the `AgentClient` and `AudioManager`. `ContentView` is now a pure, state-driven view.
*   **State Initialization:** A major UI bug where new tracks wouldn't appear was traced to the `AppState.agentState` property being `nil` when the first response from the backend arrived. The fix was to initialize `agentState` with a default, empty state in `AppState.init()`, ensuring it's always ready to be updated. This also fixed the bug where the `history_node_id` was not being updated.
*   **Audio Graph Simplicity:** An initial attempt to build a complex audio graph with parallel effect nodes proved unreliable. We refactored `AudioManager` to use a simpler, more robust serial chain for each track: `Player -> Reverb -> Delay -> MainMixer`.
*   **Audio Format Negotiation:** We resolved a runtime crash (`kAudioUnitErr_FormatNotSupported`) by allowing the `AVAudioEngine` to automatically select the best audio format between nodes instead of forcing a specific format. This was achieved by connecting nodes with a `nil` format parameter.

*   **Music Generation API:** The placeholder music generation node was successfully replaced with a full implementation using the Replicate API.
    *   **Model Versioning:** A critical `ReplicateError` ("Invalid version or not permitted") was encountered. This was resolved by replacing the outdated model hash with a specific, working version string provided by the user (`meta/musicgen:671ac645ce5e552cc63a54a2bbff63fcf798043055d2dac5fc9e36a837eedcfb`). This highlights the importance of using stable, versioned model endpoints for production systems.
    *   **Parameter Tuning:** The Replicate API call was updated to include a full set of detailed parameters from a user-provided working example, ensuring consistent and high-quality audio output. The output format was also switched from `.wav` to `.mp3` to match the model's capabilities.
    *   **Data Model Sync:** To prevent decoding errors, the Swift `Track` struct in `AgentClient.swift` was updated with optional `reverb` and `delay` properties to perfectly match the Python `Track` TypedDict, ensuring seamless state synchronization.

---

## Appendix: The Undo Tree - A Design Discussion

This section summarizes the design considerations for the undo/redo functionality.

### The Challenge: Linear vs. Branching History

A standard **linear undo** system operates like a stack. Each action is pushed onto the stack, and "undo" simply pops the last action off. If you undo and then perform a new action, the old "future" is permanently lost.

A more powerful approach is a **branching undo tree**, similar to `git`'s commit history. In this model, states are nodes in a tree. When a user undoes and then creates a new action, it creates a new *branch* in the history. The old path is not destroyed and can be revisited later. This allows for true non-destructive exploration of creative ideas.

### Cost-Benefit Analysis of a Full Undo Tree

Implementing a full undo tree with named "checkpoints" (like `git tags`) is a high-cost, high-reward feature.

*   **Complexity: Very High.** It requires building a custom version control system for the application state from the ground up, including logic for branching, navigating the tree, and handling user queries about history. The UI/UX for managing a tree visually is also a significant undertaking.
*   **Performance: Low Impact.** The runtime performance cost is minimal. Writing state snapshots to a local SQLite database is extremely fast.
*   **RAM Usage: No significant change.** Only the current state is kept in active memory.
*   **Disk Usage: Moderate Increase.** The primary cost is that audio files for undone tracks cannot be deleted, as they might be part of an alternative branch. This means every recorded or generated loop is stored permanently for the life of the project.

### The Pragmatic Compromise

Given the high implementation cost, we have adopted a **pragmatic compromise**:

1.  **Architect for a Tree, Implement as a Line:** The backend (`HistoryManager`) is being built from the ground up to support a tree structure.
2.  **Expose Simple Functionality:** For Phase 3, we will only expose a simple, linear "undo" to the user. The underlying system can handle branching, but we are not building the UI or agent logic to use it yet.

This approach avoids a massive, high-risk refactoring project in the future. When we decide to implement the full visual undo tree, the backend will be ready, and the work will be focused primarily on the frontend UI and agent interaction, making the feature much easier and cheaper to add later.

---

## Appendix: Resolving Audio Format Inconsistencies

This section details a critical bug related to audio processing and the robust solution that was implemented.

### The Problem: Pitch Shifting & Crashes

Two major issues were identified during testing:
1.  **Pitch Shifting:** Recorded audio loops would play back at a noticeably lower pitch than the original performance. This was caused by a **sample rate mismatch** between the audio recorded from the microphone and the audio being processed and played by the `AVAudioEngine`.
2.  **Channel Mismatch Crash:** When switching from the default mono microphone to a stereo microphone, the app would crash. The error message `_outputFormat.channelCount == buffer.format.channelCount` revealed that the audio player was receiving a stereo (2-channel) buffer when it was expecting a mono (1-channel) buffer, or vice-versa.

These issues pointed to a fundamental architectural problem: the app was not enforcing a consistent audio format. It was relying on the native format of the connected hardware, which could change at any time, leading to unpredictable behavior.

### The Solution: A Canonical Audio Format

To solve these problems permanently, we re-architected the `AudioManager` to enforce a single, **canonical audio format** for all audio processing.

1.  **Standardized Format:** We defined a consistent, app-wide format: **44.1kHz sample rate, stereo (2-channel)**. All audio, regardless of its source, is converted to this format.
2.  **Real-Time Conversion:** We implemented an `AVAudioConverter` directly into the recording pipeline. When the `inputNode` receives audio from the microphone, it is immediately converted from its native hardware format into our canonical format *before* being written to a file. This ensures every saved loop is standardized.
3.  **Consistent Playback:** The playback and effects chain (`AVAudioPlayerNode`, `AVAudioUnitReverb`, etc.) was also configured to operate exclusively using this canonical format.

This change ensures that the sample rate, channel count, and bit depth are identical at every stage of the audio pipeline—from recording to file storage to playback and effects processing. This resolved both the pitch-shifting bug and the channel-related crashes, making the audio system significantly more robust and reliable. 