# Phase 3 Implementation Plan: Advanced Features & Creative Tools

This document outlines the detailed plan for implementing Phase 3 of the AI Music Looper Agent. The goal of this phase is to build upon the core functionality and introduce advanced creative tools that make the application powerful and unique.

## Guiding Principles for Phase 3

*   **Component-Driven Development:** Each feature (Music Generation, Undo, etc.) will be developed as a modular component with clear responsibilities.
*   **API-First for Python:** The Python agent's new capabilities will be exposed through clean, well-defined endpoints on the FastAPI server.
*   **State-Driven UI:** The SwiftUI UI will be a direct, read-only reflection of the agent's state, updated via responses from the backend.

---

## 1. Music Generation

This feature allows the user to ask the agent to add new musical parts, which will be generated by a cloud-based AI model.

#### **Python Backend (LangGraph)**

1.  **`MusicGenerationNode` Implementation:**
    *   Create a new node file `python/nodes/music_generation_node.py`.
    *   This node will be responsible for calling an external music generation service. For the initial implementation, we can use a placeholder that returns a pre-recorded audio file to simulate the API call.
    *   The node function will take the current `State` as input to extract context like tempo (BPM) and musical key.
2.  **API Integration:**
    *   Research and select a suitable music generation API (e.g., a model from Hugging Face, or another specialized service).
    *   Implement the API client logic within the `MusicGenerationNode` to send a request with the correct parameters.
    *   Handle the API response, which will likely be an audio file (e.g., MP3 or WAV).
3.  **State Management:**
    *   When the generated audio is received, the node will save it to a temporary file, similar to how recorded loops are handled.
    *   It will then update the main `State` object by adding a new `Track` to the tracklist, including its ID, a name (e.g., "AI Bassline"), and the path to the audio file.
4.  **Agent Integration:**
    *   Add the `MusicGenerationNode` to the main LangGraph graph.
    *   Update the `RouterNode`'s LLM prompt to recognize intents like "add a drum beat" or "generate a melody" and route the workflow to the `MusicGenerationNode`.

#### **Swift Frontend (macOS)**

1.  **Triggering Generation:**
    *   No direct UI changes are needed to trigger this. The user's voice command, transcribed by `SFSpeechRecognizer`, will be sent to the Python backend.
2.  **Handling New Tracks:**
    *   The backend's response will update the Swift app's local state. The existing logic that handles adding new recorded tracks to the `AVAudioEngine` will be used to add the new AI-generated track. The track will be loaded from its file path and scheduled for playback.

#### **Testing & Verification:**

*   Speak "add a simple drum beat".
*   **Expected Outcome:** After a brief processing period, a new drum loop track is added to the session and begins playing in sync with other tracks. The UI should update to show the new track.
*   **Architectural Note:** This node must be updated to commit its final state change using the `HistoryManager`.

---

## 2. Undo/Redo Functionality (Pragmatic Compromise)

This feature implements a robust, future-proof undo system. We will build the backend to support a full branching undo tree from day one, but for Phase 3, we will only expose simple, linear undo functionality to the user. This avoids a costly future refactor.

#### **Python Backend (LangGraph)**

1.  **`HistoryManager` Implementation:**
    *   Create a new Python class `python/history_manager.py`.
    *   This class will manage the state history. It will use a simple SQLite database to store the state tree.
    *   The database will have a `state_tree` table with columns: `node_id` (PK), `parent_id` (FK to self), and `state_snapshot` (JSON blob).
    *   The `HistoryManager` will have a method: `commit(state, parent_id) -> new_id`. This writes a new state to the DB.
2.  **State Commits:**
    *   All nodes that modify the application state (e.g., `MusicGenerationNode`, `ModifyTrackNode`) will no longer mutate the state directly. At the end of their execution, they must call `history_manager.commit()` to create a new state object.
3.  **`UndoNode` Implementation:**
    *   Create an `UndoNode`. This node will not use LangGraph's checkpointer.
    *   It will call the `HistoryManager` to find the parent of the current state node and will load that parent's `state_snapshot`.
    *   This new (old) state will be passed back to the frontend, effectively performing an "undo".
4.  **Agent Integration:**
    *   Update the `RouterNode` to recognize the "undo" intent and route to the `UndoNode`. The response to the frontend will contain the entire reverted state object.

#### **Swift Frontend (macOS)**

1.  **State Synchronization:**
    *   This remains unchanged. When the Swift app receives the `newState` from the backend after an undo operation, it will replace its local state representation and re-configure the `AVAudioEngine` and UI to match. The complexity of the history tree is completely abstracted away by the backend.

#### **Testing & Verification:**

*   Record a new track.
*   Speak "undo that".
*   **Expected Outcome:** The last recorded track stops playing and is removed from the UI. The state is reverted to before the recording. The underlying database should show two states, with a parent-child relationship.

---

## 3. Creative Suggestions

The agent will be able to analyze the current musical session and provide intelligent suggestions for what to add next.

#### **Python Backend (LangGraph)**

1.  **`AnalysisNode` Implementation:**
    *   Create an `AnalysisNode`.
    *   This node will read the current `State` and generate a concise, human-readable summary of the session (e.g., "This is a 120 BPM rock track in E minor with two guitars and a drum beat.").
2.  **`SuggestionNode` Implementation:**
    *   Create a `SuggestionNode` that takes the summary from the `AnalysisNode` as input.
    *   It will use an LLM (e.g., via a LangChain chain) to generate creative suggestions based on the summary. The prompt will be engineered to elicit musically relevant ideas (e.g., "Given the following track, suggest a new instrument to add: ...").
3.  **Agent Integration:**
    *   Add the new nodes to the graph. The `RouterNode` will direct "what should I add?" intents to the `AnalysisNode`, which then flows to the `SuggestionNode`.
    *   The agent's final response will be a JSON payload containing the text to be spoken by the frontend (e.g., `{"speak": "How about adding a funky bassline?"}`).

#### **Swift Frontend (macOS)**

1.  **Spoken Response:**
    *   The existing `AVSpeechSynthesizer` integration will handle speaking the agent's suggestion to the user. No new Swift code is required for this part.

#### **Testing & Verification:**

*   Create a session with 2-3 tracks.
*   Speak "what would sound good here?".
*   **Expected Outcome:** The agent provides a spoken, relevant musical suggestion.

---

## 4. Audio Effects

Users will be able to apply common audio effects like reverb and delay to specific tracks.

#### **Swift Frontend (macOS)**

1.  **`AVAudioEngine` Enhancements:**
    *   For each track's `AVAudioPlayerNode`, attach `AVAudioUnitReverb` and `AVAudioUnitDelay` nodes between the player and the main mixer.
    *   Initially, set the `wetDryMix` for these effects to 0 so they are inactive.
2.  **Expose Effect Controls:**
    *   Update the `AudioManager` to include functions like `setReverb(trackID: String, value: Float)` and `setDelay(trackID: String, value: Float)`. These functions will adjust the `wetDryMix` property of the corresponding audio unit.
3.  **Agent Command Handling:**
    *   Enhance the network client's command handler to recognize new actions from the backend, such as `{"action": "set_reverb", "track_id": "track_001", "value": 50.0}`.

#### **Python Backend (LangGraph)**

1.  **Agent Logic:**
    *   Update the `RouterNode`'s LLM prompt to understand commands like "put some reverb on the first guitar" or "add a little delay to the last loop".
    *   The LLM must be prompted to extract the track identifier and the effect being requested.
    *   The `ModifyTrackNode` will be updated to handle these new action types and return the appropriate JSON command for the Swift frontend. This node must commit its final state using the `HistoryManager`.

#### **Testing & Verification:**

*   Record a guitar loop.
*   Speak "add a lot of reverb to that guitar".
*   **Expected Outcome:** The reverb effect is audibly applied to the guitar track.
*   **Architectural Note:** This node must commit its final state using the `HistoryManager`.

---

## 5. Basic User Interface

A minimal UI will be created to provide visual feedback of the agent's state.

#### **Swift Frontend (macOS)**

1.  **`TrackView` Component:**
    *   Create a new SwiftUI view, `TrackView.swift`, that displays information for a single track (e.g., name, volume, mute status).
2.  **Main `ContentView` Layout:**
    *   Modify `ContentView.swift` to display a list of `TrackView` components based on the tracks in the app's state.
    *   Use a `VStack` or `List` to arrange the tracks.
3.  **Status Indicators:**
    *   Add a "listening" indicator (e.g., a colored circle) that activates when `SFSpeechRecognizer` is active.
    *   Implement the dedicated Record/Stop toggle button as defined in the project overview. This button will directly call the `startRecording()` and `stopRecording()` functions in the `AudioManager`.

#### **Python Backend (LangGraph)**

1.  **State Broadcasting:**
    *   Ensure that every response from the agent that modifies the state includes the complete, updated `State` object. This is the single source of truth for the UI.

#### **Testing & Verification:**

*   Launch the application.
*   **Expected Outcome:** The main window appears with an empty state.
*   Record a loop.
*   **Expected Outcome:** A new track visualization appears in the UI.
*   Press the record button.
*   **Expected Outcome:** A "recording" indicator appears. Press it again, and the indicator disappears. 

---

## Appendix: The Undo Tree - A Design Discussion

This section summarizes the design considerations for the undo/redo functionality.

### The Challenge: Linear vs. Branching History

A standard **linear undo** system operates like a stack. Each action is pushed onto the stack, and "undo" simply pops the last action off. If you undo and then perform a new action, the old "future" is permanently lost.

A more powerful approach is a **branching undo tree**, similar to `git`'s commit history. In this model, states are nodes in a tree. When a user undoes and then creates a new action, it creates a new *branch* in the history. The old path is not destroyed and can be revisited later. This allows for true non-destructive exploration of creative ideas.

### Cost-Benefit Analysis of a Full Undo Tree

Implementing a full undo tree with named "checkpoints" (like `git tags`) is a high-cost, high-reward feature.

*   **Complexity: Very High.** It requires building a custom version control system for the application state from the ground up, including logic for branching, navigating the tree, and handling user queries about history. The UI/UX for managing a tree visually is also a significant undertaking.
*   **Performance: Low Impact.** The runtime performance cost is minimal. Writing state snapshots to a local SQLite database is extremely fast.
*   **RAM Usage: No significant change.** Only the current state is kept in active memory.
*   **Disk Usage: Moderate Increase.** The primary cost is that audio files for undone tracks cannot be deleted, as they might be part of an alternative branch. This means every recorded or generated loop is stored permanently for the life of the project.

### The Pragmatic Compromise

Given the high implementation cost, we have adopted a **pragmatic compromise**:

1.  **Architect for a Tree, Implement as a Line:** The backend (`HistoryManager`) is being built from the ground up to support a tree structure.
2.  **Expose Simple Functionality:** For Phase 3, we will only expose a simple, linear "undo" to the user. The underlying system can handle branching, but we are not building the UI or agent logic to use it yet.

This approach avoids a massive, high-risk refactoring project in the future. When we decide to implement the full visual undo tree, the backend will be ready, and the work will be focused primarily on the frontend UI and agent interaction, making the feature much easier and cheaper to add later. 